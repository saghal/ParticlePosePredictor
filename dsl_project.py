# -*- coding: utf-8 -*-
"""dsl_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WrBhXjoPtWBjH-RA5eGZxeXVt-EMGMUb

## Installing requirements and download and preparing the dataset
"""

from google.colab import drive
drive.mount('/content/drive')

!cp /content/drive/MyDrive/DSL_Winter_Project_2024.zip ./

# !gdown https://drive.google.com/uc?id=1RqoGwJ7lkcXc7INE9hINUQZW2O8ltBcB

!unzip -q ./DSL_Winter_Project_2024.zip
!mv ./DSL_Winter_Project_2024/development.csv .
!mv ./DSL_Winter_Project_2024/evaluation.csv .
!mv ./DSL_Winter_Project_2024/sample_submission.csv .
!rm -rf /content/__MACOSX ./DSL_Winter_Project_2024 ./DSL_Winter_Project_2024.zip

import pandas as pd
import numpy as np
import time
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.neural_network import MLPRegressor
import matplotlib.pyplot as plt
import seaborn as sns

df_development = pd.read_csv("development.csv");
df_evaluation = pd.read_csv("evaluation.csv")

"""### Plotting X and Y of each row."""

df_development.plot.scatter(x="x",y="y",alpha=0.5)

"""### x and y range from 200 to 600 and are multiply of 5"""

df_development['x'].unique()

df_development['y'].unique()

"""## Data preparation

### 1. Removing noise and unimportant columns

First we plotted all columns of all features using box plots, we found out that the behavior of pads number 0, 7, 12, 15, 16, 17 are different from others in most of sub-features including pmax, area, tmax, so probably we can say that they are noises. but we can't definitely say which features are noises and which are not, and it is a complex task.

behavior of some features like rms is different and we can not have illusion from box plot
hence, we tried some numeric based methods like calculating the **Variance** and **Median Absolute Deviation (MAD)** of each feacture to select the N largests in each category (for example all tmax columns) as the noise columns. But the result of each category of features were completely different.
From this two way we can understand that some pads like number 17, 16, 15 are probably unimportant.

Then we used two key tool as measures to solve the problem. **Mutual Information (MI)** and **Feature Importance Score in Random Forest** and we decided to focus on select the best useful features instead of removing unimportant ones.

The explanation for each of the measures is in it's part â†“

#### Box plots
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

boxplot_color = 'skyblue'
features = ['pmax', 'negpmax', 'area', 'tmax', 'rms']

for f in features:
    columns = [col for col in df_development.columns if col.startswith(f)]
    fig, ax = plt.subplots(figsize=(12, 7))

    boxprops = dict(linewidth=2, color=boxplot_color)
    medianprops = dict(linestyle='-', linewidth=2.5, color='black')  # Black median line
    whiskerprops = dict(linestyle='-', linewidth=1.5, color='black')  # Black whiskers
    capprops = dict(linewidth=1.5, color='black')  # Black caps

    ax.boxplot(df_development[columns], positions=np.arange(1, len(columns) + 1), vert=False, boxprops=boxprops,
               medianprops=medianprops, whiskerprops=whiskerprops, capprops=capprops)

    ax.set_yticks(np.arange(1, len(columns) + 1))
    ax.set_yticklabels(columns)

    ax.grid(axis='x', linestyle='--', alpha=0.7)

    ax.set_title(f'Boxplot for {f}', fontsize=16)
    ax.set_xlabel('Values', fontsize=14)
    ax.set_ylabel('Features', fontsize=14)

    plt.tight_layout()
    plt.show()
    print('\n')

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

boxplot_color = 'skyblue'
feature = 'pmax'

columns = [col for col in df_development.columns if col.startswith(feature)]
fig, ax = plt.subplots(figsize=(12, 7))

boxprops = dict(linewidth=2, color=boxplot_color)
medianprops = dict(linestyle='-', linewidth=2.5, color='black')  # Black median line
whiskerprops = dict(linestyle='-', linewidth=1.5, color='black')  # Black whiskers
capprops = dict(linewidth=1.5, color='black')  # Black caps

ax.boxplot(df_development[columns], positions=np.arange(1, len(columns) + 1), vert=True, boxprops=boxprops,
            medianprops=medianprops, whiskerprops=whiskerprops, capprops=capprops)

ax.set_xticks(np.arange(1, len(columns) + 1))
ax.set_xticklabels(list(range(18)))

ax.grid(axis='x', linestyle='--', alpha=0.7)

ax.set_title(f'Boxplot for {f}', fontsize=16)
ax.set_ylabel('Values', fontsize=14)
ax.set_xlabel('Pmax features', fontsize=14)

plt.tight_layout()
plt.show()
print('\n')

"""#### Mutual Information Scores (MI)

In these 3 cells, we calculated the MI score betwen each column and x, y in our dataset and sort them based on their categories.
And in the end we select the best columns by setting a threshold (MI > 0.1).

**The important point is that we calculated MI score of each column one time separately with x and then with y.
Because we believe that by predicting value for x and y separately we will have a less complex model for each of them, so we can achieve a better result.**
"""

xfeatures_mi_info = [] # features Sorted by mutual information scores with target x (including their mi socre and rank, also categorized due to their category (pmax, tmax ...))
from sklearn.feature_selection import SelectKBest, mutual_info_regression

X = df_development.drop(columns=['x', 'y'])
selectorx = SelectKBest(score_func=mutual_info_regression, k='all')
selectorx.fit(X, df_development['x'])
mi_scoresx = selectorx.scores_
# sort features by mutual information scores
sorted_featuresx = sorted(zip(X.columns, mi_scoresx), key=lambda x: x[1], reverse=True)
xfeatures_mi_info.append(sorted_featuresx)
# print the rank and value of mutual information for each feature
for rank, (feature, mi_value) in enumerate(sorted_featuresx, start=1):
    print(f"Rank {rank}: Feature '{feature}' - Mutual Information: {mi_value:.4f}")

yfeatures_mi_info = [] # features Sorted by mutual information scores with target x (including their mi socre and rank, also categorized due to their category (pmax, tmax ...))
from sklearn.feature_selection import SelectKBest, mutual_info_regression

X = df_development.drop(columns=['x', 'y'])
selectory = SelectKBest(score_func=mutual_info_regression, k='all')
selectory.fit(X, df_development['y'])
mi_scoresy = selectory.scores_
# sort features by mutual information scores
sorted_featuresy = sorted(zip(X.columns, mi_scoresy), key=lambda x: x[1], reverse=True)
yfeatures_mi_info.append(sorted_featuresy)
# print the rank and value of mutual information for each feature
for rank, (feature, mi_value) in enumerate(sorted_featuresy, start=1):
    print(f"Rank {rank}: Feature '{feature}' - Mutual Information: {mi_value:.4f}")

# Commented out IPython magic to ensure Python compatibility.
features = [row[0] for row in sorted_featuresx]
mi_values = [row[1] for row in sorted_featuresx]

plt.figure(figsize=(3, 10))
# %matplotlib inline

sns.barplot(x=mi_values, y=features, orient="h")

ax = plt.gca()
for tick, value in zip(ax.get_yticklabels(), mi_values):
    tick.set_color('red' if value < 0.02 else 'black')  # 'black' for default color

plt.yticks(fontsize=7)

plt.xlabel('Score')
plt.ylabel('Features')
plt.title("MI scores features (X)")

plt.show()

# Commented out IPython magic to ensure Python compatibility.
features = [row[0] for row in sorted_featuresy]
mi_values = [row[1] for row in sorted_featuresy]

plt.figure(figsize=(3, 10))
# %matplotlib inline

sns.barplot(x=mi_values, y=features, orient="h")

ax = plt.gca()
for tick, value in zip(ax.get_yticklabels(), mi_values):
    tick.set_color('red' if value < 0.02 else 'black')  # 'black' for default color

plt.yticks(fontsize=7)

plt.xlabel('Score')
plt.ylabel('Features')
plt.title("MI scores features (Y)")

plt.show()

# Commented out IPython magic to ensure Python compatibility.
columns = [col for col in df_development.columns if col.startswith('negpmax')]
features = []
mi_values = []
for rank, (feature, mi_value) in enumerate(sorted_featuresx, start=1):
  if feature in columns:
    mi_values.append(mi_value)
    features.append(feature)

plt.figure(figsize=(3, 3))
# %matplotlib inline

sns.barplot(x=mi_values, y=features, orient="h")

ax = plt.gca()
for tick, value in zip(ax.get_yticklabels(), mi_values):
    tick.set_color('red' if value < 0.02 else 'black')  # 'black' for default color

plt.yticks(fontsize=7)

plt.xlabel('Score')
plt.ylabel('Features')
plt.title("MI scores for negpmax features (X)")

plt.show()

# select feactures with mi score more than mi_threshold
mi_threshold = 0.02
selected_featuresx = []
for features_list in xfeatures_mi_info:
  for (feature, mi_value) in features_list:
    if mi_value > mi_threshold:
      selected_featuresx.append(feature)

selected_featuresy = []
for features_list in yfeatures_mi_info:
  for (feature, mi_value) in features_list:
    if mi_value > mi_threshold:
      selected_featuresy.append(feature)

print(f'Number of selected features for predicting X: {len(selected_featuresx)}\nFeatures:{selected_featuresx}')
print(f'Number of selected features for predicting Y: {len(selected_featuresy)}\nFeatures:{selected_featuresy}')

"""#### Random Forest Feature Importance

In this part we created two models (one for x and one for y) using the feactures selected using MI > 0.1 and then we plotted the importance of features.

Then we select the features with importance > 0.001 as our final selection for training our model.

(We tried a lot of combination for thresholds and algorithm for feature selection in this project, and the things we wrote hear, are the best out of them.)
"""

# create a Random Forest regressor on the features selected for predicting x , y
from sklearn.ensemble import RandomForestRegressor
x_rf_regressor = RandomForestRegressor(n_estimators=100, max_features='sqrt',
                                       bootstrap=True, min_samples_split=2, max_depth=None, random_state=42)
y_rf_regressor = RandomForestRegressor(n_estimators=100, max_features='sqrt',
                                       bootstrap=True, min_samples_split=2, max_depth=None, random_state=42)

# train the model
x_rf_regressor.fit(df_development[selected_featuresx], df_development['x'])
y_rf_regressor.fit(df_development[selected_featuresy], df_development['y'])

feature_impx = pd.Series(x_rf_regressor.feature_importances_,index=selected_featuresx).sort_values(ascending=False)
feature_impy = pd.Series(y_rf_regressor.feature_importances_,index=selected_featuresy).sort_values(ascending=False)

# Commented out IPython magic to ensure Python compatibility.
plt.figure(figsize=(3, 5),dpi=200)
# %matplotlib inline

sns.barplot(x=feature_impx, y=feature_impx.index, orient="h", palette=bar_colors)

ax = plt.gca()
for tick, value in zip(ax.get_yticklabels(), feature_impx):
    tick.set_color('red' if value < 0.001 else 'black')  # 'black' for default color

plt.yticks(fontsize=7)

# Add labels to the graph
plt.xlabel('Importance')
plt.ylabel('Features')
plt.title("Random forest feature importance (X)")

plt.show()

# Commented out IPython magic to ensure Python compatibility.
plt.figure(figsize=(3, 5),dpi=200)
# %matplotlib inline

sns.barplot(x=feature_impy, y=feature_impy.index, orient="h", palette=bar_colors)

ax = plt.gca()
for tick, value in zip(ax.get_yticklabels(), feature_impy):
    tick.set_color('red' if value < 0.001 else 'black')  # 'black' for default color

plt.yticks(fontsize=7)

plt.xlabel('Importance')
plt.ylabel('Features')
plt.title("Random forest feature importance (Y)")

plt.show()

# choose the features that their importance is more than the threshold (0.001)
threshold = 0.001

# X
selected_featuresx = list(feature_impx[feature_impx > threshold].index)
print(f'Number of selected features for predicting X: {len(selected_featuresx)}\nFeatures:{selected_featuresx}')

# Y
selected_featuresy = list(feature_impy[feature_impy > threshold].index)
print(f'Number of selected features for predicting Y: {len(selected_featuresy)}\nFeatures:{selected_featuresy}')

"""### 2.Standardizing the data

Next, We standardize all columns (except x and y) of two DataFrames, df_development and df_evaluation, using scikit-learn's StandardScaler.
In the end, we have df_train (includes standardaized data of df_development for training), df_label (includes x and y for each row of df_train), and df_evaluation_scaled (standardied format of df_evaluation).
"""

df_train = df_development.drop(columns=['x', 'y'])
df_label = df_development[['x', 'y']]
scaler = StandardScaler()
df_train = pd.DataFrame(scaler.fit_transform(df_train), columns=df_train.columns)
df_evaluation_scaled = pd.DataFrame(scaler.transform(df_evaluation.drop(columns=['Id'])), columns=df_evaluation.columns[1:])


df_evaluation_scaled['Id'] = df_evaluation['Id']

df_train

df_label

df_evaluation_scaled

"""## Training methods

As we mentioned in the Mutual Information Scores section (Data preparation), we decided to train two separate models, one for x and one for y. because in our opinion, we can see the prediction of x and y as two independent problems.

By doing so we will have two simpler problems to solve, and our trained model will be less complex.

In the cells below we will train two MLP model (for x and y). We have tested a couple of algorithms (Random Forest Regressor, MLP, and ensemble MLP)
 and use trial and and error to calculate best tuning and achieve the best performance.
But using ensenble MLP we could achieve our best average Euclidean distance 3.875.

#### Random Forest Regressor :
The result (average euclidean distance) for evaluation dataset on the leaderboard website (http://trinidad.polito.it:8888/) was **4.824** for this approach.
"""

# create a Random Forest regressor on the features selected for predicting x
x_rf_regressor = RandomForestRegressor(n_estimators=100, max_features='sqrt',
                                       bootstrap=True, min_samples_split=2, max_depth=None, random_state=42)
y_rf_regressor = RandomForestRegressor(n_estimators=100, max_features='sqrt',
                                       bootstrap=True, min_samples_split=2, max_depth=None, random_state=42)

# train the model
x_rf_regressor.fit(df_development[selected_featuresx], df_development['x'])
y_rf_regressor.fit(df_development[selected_featuresy], df_development['y'])

x_preds = x_rf_regressor.predict(df_evaluation[selected_featuresx])
y_preds = y_rf_regressor.predict(df_evaluation[selected_featuresy])

# create a new DataFrame for the submission file
submission_df = pd.DataFrame({'Id': df_evaluation['Id'], 'Predicted': [f"{x}|{y}" for x, y in zip(x_preds, y_preds)]})

submission_df.to_csv('submission_randomforest.csv', index=False)

"""#### Multilayer perceptron (MLP) :
We trained our MLP model with these hyperparameters:
  1. hidden_layer_sizes=(128, 64, 32) : we have tested different numbers of hidden layers, for example the one more complex model with 4 hidden layers (256, 128, 64, 32) but the result was not good enough.
  2. activation='tanh' : previously we used relu activation function but as you know that our data after applying scaling function has some negative numbers, so the best option is tanh.
  3. max_iter=70 : After decreasing this hyperparameter from 100 and 200 to 50, the model performed absolutely better, because it prevented overfitting. Of course, less max_iter led to underfitting.
  4. learning_rate='adaptive': The result was better than using 'constant' and 'invscaling'.
  5. alpha and learning_rate_initial are default (0.0001 and 0.001 respectively).

The result (average euclidean distance) for evaluation dataset on the leaderboard website (http://trinidad.polito.it:8888/) was 4.251 for this approach.
"""

modelx = MLPRegressor(hidden_layer_sizes=(128, 64, 32), activation='tanh', solver='adam', random_state=42,
        batch_size='auto', learning_rate='adaptive', max_iter=70, verbose=True)
modelx.fit(df_train[selected_featuresx], df_label['x'])

modely = MLPRegressor(hidden_layer_sizes=(128, 64, 32), activation='tanh', solver='adam', random_state=42,
        batch_size='auto', learning_rate='adaptive', max_iter=70, verbose=True)
modely.fit(df_train[selected_featuresy], df_label['y'])

x_preds = modelx.predict(df_evaluation_scaled[selected_featuresx])
y_preds = modely.predict(df_evaluation_scaled[selected_featuresy])

# create a new DataFrame for the submission file
submission_df = pd.DataFrame({'Id': df_evaluation['Id'], 'Predicted': [f"{x}|{y}" for x, y in zip(x_preds, y_preds)]})

submission_df.to_csv('submission_mlp.csv', index=False)

"""#### Ensemble MLP :
We have used 4 MLP model for each x and y to achieve better performance and avoiding overfitting.

The result (average euclidean distance) for evaluation dataset on the leaderboard website (http://trinidad.polito.it:8888/) was 3.98 for this approach.
"""

# create and train multiple MLP models
modelsx = [
    MLPRegressor(hidden_layer_sizes=(100, 50), activation='tanh', solver='adam',
        batch_size='auto', learning_rate='adaptive', max_iter=65, verbose=True),
    MLPRegressor(hidden_layer_sizes=(100, 50), activation='tanh', solver='adam',
        batch_size='auto', learning_rate='adaptive', max_iter=65, verbose=True),
    MLPRegressor(hidden_layer_sizes=(128, 64, 32), activation='tanh', solver='adam',
        batch_size='auto', learning_rate='adaptive', max_iter=70, verbose=True),
    MLPRegressor(hidden_layer_sizes=(128, 64, 32), activation='tanh', solver='adam',
        batch_size='auto', learning_rate='adaptive', max_iter=70, verbose=True),
    ]

for model in modelsx:
    model.fit(df_train[selected_featuresx], df_label['x'])

# create and train multiple MLP models
modelsy = [
    MLPRegressor(hidden_layer_sizes=(100, 50), activation='tanh', solver='adam',
        batch_size='auto', learning_rate='adaptive', max_iter=65, verbose=True),
    MLPRegressor(hidden_layer_sizes=(100, 50), activation='tanh', solver='adam',
        batch_size='auto', learning_rate='adaptive', max_iter=65, verbose=True),
    MLPRegressor(hidden_layer_sizes=(128, 64, 32), activation='tanh', solver='adam',
        batch_size='auto', learning_rate='adaptive', max_iter=70, verbose=True),
    MLPRegressor(hidden_layer_sizes=(128, 64, 32), activation='tanh', solver='adam',
        batch_size='auto', learning_rate='adaptive', max_iter=70, verbose=True),
    ]

for model in modelsy:
    model.fit(df_train[selected_featuresy], df_label['y'])

predictions = [model.predict(df_evaluation_scaled[selected_featuresx]) for model in modelsx]
x_preds = np.mean(predictions, axis=0)
predictions = [model.predict(df_evaluation_scaled[selected_featuresy]) for model in modelsy]
y_preds = np.mean(predictions, axis=0)

submission_df = pd.DataFrame({'Id': df_evaluation['Id'], 'Predicted': [f"{x}|{y}" for x, y in zip(x_preds, y_preds)]})

submission_df.to_csv('submission_ensemble_mlp.csv', index=False)

"""#### Ensemble MLP with rounding strategy (best and final aproach) :
in this part we round a each predicted x and y using Ensemble MLP to the nearest multiple of 5.

The result (average euclidean distance) for evaluation dataset on the leaderboard website (http://trinidad.polito.it:8888/) was 3.851 for this approach.
"""

import pandas as pd
import math

df = pd.read_csv('submission_ensemble_mlp.csv')

# define a function to round a number to the nearest multiple of 5
def round_to_5(x):
    return round(x / 5) * 5

df[['x', 'y']] = df['Predicted'].str.split('|', expand=True).astype(float)
df['x'] = df['x'].apply(round_to_5)
df['y'] = df['y'].apply(round_to_5)

df['Predicted'] = df['x'].astype(str) + '|' + df['y'].astype(str)

# drop the 'x' and 'y' columns
df.drop(columns=['x', 'y'], inplace=True)

# the final submission file
df.to_csv('best_submission_Ensemble_rounded.csv', index=False, header=['Id', 'Predicted'])

"""## Comparison between models performance"""

import matplotlib.pyplot as plt

# Data
models = ['Random Forest', 'Multilayer Perceptron', 'Ensemble MLP', 'Ensemble MLP Rounded']
euclidean_distances = [4.824, 4.251, 3.98, 3.851]

# Sorting the models by their Euclidean distances in descending order
sorted_indices_desc = sorted(range(len(euclidean_distances)), key=lambda k: euclidean_distances[k], reverse=True)
sorted_models_desc = [models[i] for i in sorted_indices_desc]
sorted_euclidean_distances_desc = [euclidean_distances[i] for i in sorted_indices_desc]

# Creating the line chart
plt.figure(figsize=(8, 5))
plt.plot(sorted_models_desc, sorted_euclidean_distances_desc, marker='o', linestyle='-', color='blue')
plt.xlabel('Model')
plt.ylabel('Average Euclidean Distance')
plt.title('Average Euclidean Distance of Models on the Evaluation Set')
plt.grid(True)
plt.yticks(sorted_euclidean_distances_desc)

plt.show()